---
title: "ProjectFinal_Allen_Nicholas"
output: html_document
Subject:    Project Final
Class:      DSCI 512
Section:    1W 24/SP2    
Instructor: Nengbing Tao
date: "2024-04-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### 1. Data Preparation
#### * a. Load the dataset insurance.csv Download insurance.csv into memory.
```{r}
Insurance <- read.csv("C:/Users/user1/OneDrive/Documents/DSCI 512 Predictive Modelling/Week 8/insurance.csv", header=T)
View(Insurance)
head(Insurance,5) #printed in R markdown html for proof
```

### 1.b In the data frame, transform the variable charges by setting insurance$charges = log(insurance$charges). Do not transform it outside of the data frame.
```{r}
if (!require("data.table")) install.packages("data.table")
if (!require("data.table")) library(data.table) #necessary for copy() function
Insurance_log = copy(Insurance)
Insurance_log$charges = log(Insurance_log$charges)
View(Insurance_log)
head(Insurance_log,5) #printed in R markdown html for proof
```

### 1.c Using the data set from 1.b, use the model.matrix() function to create another data set that uses dummy variables in place of categorical variables. Verify that the first column only has ones (1) as values, and then discard the column only after verifying it has only ones as values.
```{r}
Insurance_1c = model.matrix(~.,data=Insurance_log)
View(Insurance_1c)
head(Insurance_1c,5) #printed in R markdown html for proof
```

```{r}
Insurance_1c = Insurance_1c[,2:9]
View(Insurance_1c)
head(Insurance_1c,5) #printed in R markdown html for proof
```

### 1.d Use the sample() function with set.seed equal to 1 to generate row indexes for your training and tests sets, with 2/3 of the row indexes for your training set and 1/3 for your test set. Do not use any method other than the sample() function for splitting your data. 
```{r}
set.seed(1)
index_general = sample(1:nrow(Insurance_1c),2*nrow(Insurance_1c)/3)
```

### 1.e Create a training and test data set from the data set created in 1.b using the training and test row indexes created in 1.d. Unless otherwise stated, only use the training and test data sets created in this step.
```{r}
train_general = Insurance_log[index_general,]
test_general = Insurance_log[-index_general,]
```


### 1.f Create a training and test data set from data set created in 1.c using the training and test row indexes created in 1.d
```{r}
train_1f = Insurance_1c[index_general,]
test_1f = Insurance_1c[-index_general,]
```


### 2. Build a multiple linear regression model.
#### * a. Perform multiple linear regression with charges as the response and the predictors are age, sex, bmi, children, smoker, and region. Print out the results using the summary() function. Use the training data set created in step 1.e to train your model.
```{r}
lm.fit = lm(charges~age+sex+bmi+children+smoker+region,data=train_general)
```


### 2.b Is there a relationship between the predictors and the response?
```{r}
summary(lm.fit)
```
Since the p-value is far less than 5% (2.2E-16) we can say there is clear evidence to reject the NULL hypothesis. This means that there is evidence of a relationship between the predictors and the target 'charges'.

### 2.c Does sex have a statistically significant relationship to the response?
To determine which of multiple predictors have a meaningful relationship to the response, we must look at the individual p values displayed for each predictor. These are denoted in the Pr(>|t|) column of the summary output. Statistical significance is determined as noticeably below 5%. The sex feature is almost half of the 5%. I would consider this noticeably below our
threshold and thus, statistically significant. 

### 2.d Perform best subset selection using the stepAIC() function from the MASS library, choose best model based on AIC. For the "direction" parameter in the stepAIC() method, set direciton="backward"
```{r}
if (!require("MASS")) install.packages("MASS")
if (!require("MASS")) library(MASS)
#1st arg is the full model result, 2nd is the direction. It could be backward or forward etc.
#can use lm.fit for full model since it already uses all features
lm.bwd = stepAIC(lm.fit, direction = "backward")
#call backward model to see which predictors produce the best model
lm.bwd
```

### 2.e Compute the test error of the best model in #2d based on AIC using LOOCV using trainControl() and train() from the caret library. Report the MSE by squaring the reported RMSE.
```{r}
if (!require("caret")) install.packages("caret")
if (!require("caret")) library(caret)
train_ctrl_method_2e = trainControl(method="LOOCV")
model_2e <-train(charges~age+sex+bmi+children+smoker+region,data=test_general, trControl=train_ctrl_method_2e, method="lm")
print(model_2e)
#attempted manual error calculation
#mean((test_general$charges - predict.lm(model_2e, test_general)) ^ 2)
Test.MSE_2e = (model_2e$results$RMSE)^2
Test.MSE_2e
```

### 2.f Calculate the test error of the best model in #2d based on AIC using 10-fold Cross-Validation. Use train and trainControl from the caret library. Refer to model selected in #2d based on AIC. Report the MSE.
```{r}
train_ctrl_method_2f = trainControl(method="CV", number=10)
#MUST SWITCH TO Test DATA SET
model_2f <-train(charges~age+sex+bmi+children+smoker+region,data=test_general, trControl=train_ctrl_method_2f, method="lm")
print(model_2f)
#attempted manual error calculation
#mean((test_general$charges - predict.lm(model_2f, test_general)) ^ 2)
Test.MSE_2f = (model_2f$results$RMSE)^2
Test.MSE_2f
```

### 2.g Calculate and report the test MSE using the best model from 2.d and test data set created in step 1.e.
```{r}
train_ctrl_method_2e = trainControl(method="LOOCV")
model_2e <-train(charges~age+sex+bmi+children+smoker+region,data=test_general, trControl=train_ctrl_method_2e, method="lm")
print(model_2e)
#attempted manual error calculation
#mean((test_general$charges - predict.lm(model_2e, test_general)) ^ 2)
Test.MSE_2g = (model_2e$results$RMSE)^2
Test.MSE_2g
```

### 2.h Compare the test MSE calculated in step 2.f using 10-fold cross-validation with the test MSE calculated in step 2.g. How similar are they?
22.9% compared to 23.4% are very similar meaning the validation methods are equally effective. 

### 3.Build a regression tree model.
#### * a Build a regression tree model using function tree(), where charges is the response and the predictors are age, sex, bmi, children, smoker, and region.
```{r}
if (!require("tree")) install.packages("tree")
if (!require("tree")) library(tree)
tree.Insurance=tree(charges~age+sex+bmi+children+smoker+region, train_general)
summary(tree.Insurance)
```

### 3.b Find the optimal tree by using cross-validation and display the results in a graphic. Report the best size.
```{r}
cv.Insurance = cv.tree(tree.Insurance)
plot(cv.Insurance$size, cv.Insurance$dev, ttpe = 'b')
plot(tree.Insurance)
text(tree.Insurance, pretty = 0)
```

Size 5 still does not seem like a super complex model so we will use 5. 

### 3.c Justify  the number you picked for the optimal tree with regard to the principle of variance-bias trade-off.
While in the previous section we saw that the best size is 5, the bias-variance
trade off teaches us to pick the point at the bottom of the U curve. Therefore,
I think size 3 is a better choice as it is a good balance between complexity 
as well as flexibility. 

### 3.d Prune the tree using the optimal size found in 3.b 
```{r}
#From 3b the best size is 5
prune.Insurance = prune.tree(tree.Insurance, best =5)
```

### 3.e Plot the best tree model and give labels.
```{r}
plot(prune.Insurance)
text(prune.Insurance, pretty = 0)
```

### 3.f Calculate the test MSE for the best model. 
```{r}
Predictions = predict(prune.Insurance, newdata = test_general)
Insurance.test = test_general[, "charges"]

Test.MSE_3f = mean((Predictions-Insurance.test)^2)
Test.MSE_3f
```

### 4. Build a random forest model. 
#### *a Build a random forest model using function randomForest(), where charges is the response and the predictors are age, sex, bmi, children, smoker, and region.
```{r}
if (!require("randomForest")) install.packages("randomForest")
if (!require("randomForest")) library(randomForest)
rf.Insurance=randomForest(charges~.,data=train_general,importance=TRUE)
```

### 4.b Compute the test error using the test data set.
```{r}
Predictions.rf = predict(rf.Insurance, newdata = test_general)
Test.MSE_4b = mean((Predictions.rf - Insurance.test)^2)
Test.MSE_4b
```

### 4.c Extract variable importance measure using the importance() function.
```{r}
importance(rf.Insurance)
```

### 4.d Plot the variable importance using the function, varImpPlot(). Which are the top 3 important predictors in this model?
```{r}
varImpPlot(rf.Insurance)
```
smoker and age are the 2 variables that make the most impact in our model. The 3rd 
is a fight between children and bmi but bmi seems to be farther ahead in the 
IncNodePurity metric and thus it seems the logical choice as 3rd most important predictor. 

### 5. Build a support vector machine model
#### *a The response is charges and the predictors are age, sex, bmi, children, smoker, and region. Please use the svm() function with radial kernel and gamma=5 and cost = 50.
```{r}
if (!require("e1071")) install.packages("e1071")
if (!require("e1071")) library(e1071)
svm.fit=svm(charges~age+sex+bmi+children+smoker+region,data=train_general,kernel="radial",gamma=5,cost=50)
summary(svm.fit)
```


### 5.b Perform a grid search to find the best model with potential cost: 1, 10, 50, 100 and potential gamma: 1,3 and 5 and potential kernel: "linear","radial" and "sigmoid". And use the training set created in step 1.e.
```{r}
tune.out = tune(svm,charges~age+sex+bmi+children+smoker+region,data=train_general, 
                ranges = list(cost = c(1, 10, 50, 100), gamma= c(1, 3, 5),kernel=c("linear","radial","sigmoid")))
```

### 5.c Print out the model results. What are the best model parameters?
```{r}
summary(tune.out)
```
The best model parameters happen to be 1 for both cost and gamma with a radial kernel.

### 5.d Forecast charges using the test dataset and the best model found in c).
```{r}
prediction = predict(tune.out$best.model, newdata = test_general)
```

### 5.e Compute the MSE (Mean Squared Error) on the test data.
```{r}
#get correct labels
trueObservation_Insurance= test_general[, "charges"]
#cant use confusion matrix because our target is not binary
#table(trueObservation_Insurance, prediction)

Test.MSE_5e = mean((prediction-trueObservation_Insurance)^2)
Test.MSE_5e
```

### 6. Perform the k-means cluster analysis.
#### * a Remove the sex, smoker, and region, since they are not numerical values.
```{r}
Insurance_kmc = subset(Insurance_log,select = -c(sex,smoker,region))
View(Insurance_kmc)
head(Insurance_kmc,5) #printed in R markdown html for proof
```

### 6.b Determine the optimal number of clusters. Justify your answer. It may take longer running time since it uses a large dataset.
```{r}
if (!require("factoextra")) install.packages("factoextra")
if (!require("factoextra")) library(factoextra)
if (!require("cluster")) install.packages("cluster")
if (!require("cluster")) library(cluster)
fviz_nbclust(Insurance_kmc, kmeans, method = "gap_stat")
```
I would choose 2 clusters at it has the highest gap statistic.  

### 6.c Perform k-means clustering using the 3 clusters.
```{r}
km.res <- kmeans(Insurance_kmc, 3, nstart = 25)
```

### 6.d Visualize the clusters in different colors.
```{r}
fviz_cluster(km.res, data = Insurance_kmc)
```

### 7. Build a neural networks model.
#### *a Remove the sex, smoker, and region, since they are not numerical values.
```{r}
Insurance_nn = subset(Insurance,select = -c(sex,smoker,region)) #try using non log transformed dataset
View(Insurance_nn)
head(Insurance_nn,5) #printed in R markdown html for proof
```

### 7.b Standardize the inputs using the scale() function.
```{r}
scaled.Insurance_nn =  scale(Insurance_nn)
```

### 7.c Convert the standardized inputs to a data frame using the as.data.frame() function.
```{r}
scaled.Insurance_nn = as.data.frame(scaled.Insurance_nn)
View(scaled.Insurance_nn)
head(scaled.Insurance_nn,5) #printed in R markdown html for proof
```

### 7.d Split the dataset into a training set containing 80% of the original data and the test set containing the remaining 20%.
```{r}
index = sample(1:nrow(scaled.Insurance_nn),0.8*nrow(scaled.Insurance_nn))
train_nn = scaled.Insurance_nn[index,]
test_nn = scaled.Insurance_nn[-index,]
```

### 7.e The response is charges and the predictors are age, bmi, and children. Please use 1 hidden layer with 1 neuron.
```{r}
if (!require("neuralnet")) install.packages("neuralnet")
if (!require("neuralnet")) library(neuralnet)
#should we be using the training data from #7.d? It does not explicitly instruct to. 
nn.model<-neuralnet(charges~age+bmi+children,data=train_general,hidden=1)
```

### 7.f Plot the neural networks.
```{r}
plot(nn.model, rep="best")
```

### 7.g Forecast the charges in the test dataset.
```{r}
predict.nn = compute(nn.model, test_nn[, c("age", "bmi", "children")])
```

### 7.h Get the observed charges of the test dataset.
```{r}
observ.test = test_nn$charges
```

### 7.i Compute test error (MSE).
```{r}
Test.MSE_7i = mean((observ.test- predict.nn$net.result)^2)
Test.MSE_7i
```

### 8.a For predicting insurance charges, your supervisor asks you to choose the best model among the multiple regression, regression tree, random forest, support vector machine, and neural network models. Compare the test MSEs of the models generated in steps 2.g, 3.f, 4.b, 5.e, and 7.d. Display the names for these types of these models, using these labels: Multiple Linear Regression, Regression Tree, Random Forest, Support Vector Machine, and Neural Network and their corresponding test MSEs in a data.frame. Label the column in your data frame with the labels as Model.Type, and label the column with the test MSEs as Test.MSE and round the data in this column to 4 decimal places. Present the formatted data to your supervisor and recommend which model is best and why.
```{r}
Problem.Number <- c("2.g", "3.f", "4.b", "5.e", "7.i")
Model.Type <- c("Multiple Linear Regression", "Regression Tree", "Random Forest", "Support Vector Machine", "Neural Network")
Test.MSE <- round(c(Test.MSE_2g, Test.MSE_3f, Test.MSE_4b, Test.MSE_5e, Test.MSE_7i), digits = 4)
BestModel_df = data.frame(Problem.Number,Model.Type,Test.MSE)
View(BestModel_df)
BestModel_df
```

The random forest is the best model for having the lowest error by quite some margin 
while also being fairly simple to implement. 
### 8.b Another supervisor from the sales department has requested your help to create a predictive model that his sales representatives can use to explain to clients what the potential costs could be for different kinds of customers, and they need an easy and visual way of explaining it. What model would you recommend, and what are the benefits and disadvantages of your recommended model compared to other models?
I would still recommend the random forest model because it is the average of numerous
decision tree models. Since its constituents are individual decision trees, there
is still a strong visual component and can be explained fairly easily or understood
fairly intuitively. However, the addition of averaging across multiple trees helps 
to curb one of decision trees main disadvantages (its propensity to over fit or be 
sensitive to outliers). Random forests are still relatively fast when compared to a
support vector machine or neural network. Forests can also handle regression or 
classification as well as non-linearity (thus extending beyond the reaches of linear 
regression or logistic regression). Forests furthermore still provide worthwhile 
results even when the training data set is less than large, which cannot be said 
for neural networks or SVM's, both benefiting more with sufficiently sized data. 

### 8.c The supervisor from the sales department likes your regression tree model. But she says that the salespeople say the numbers in it are way too low and suggests that maybe the numbers on the leaf nodes predicting charges are log transformations of the actual charges. You realize that in step 1.b of this project that you had indeed transformed charges using the log function. And now you realize that you need to reverse the transformation in your final output. The solution you have is to reverse the log transformation of the variables in the regression tree model you created and redisplay the result. 

#### 8.c.i Copy your pruned tree model to a new variable.
```{r}
if (!require("data.table")) install.packages("data.table")
if (!require("data.table")) library(data.table)
prune.Insurance_antilog = copy(prune.Insurance)
```

#### 8.c.ii In your new variable, find the data.frame named "frame" and reverse the log transformation on the data.frame column yval using the exp() function. (If the copy of your pruned tree model is named copy_of_my_pruned_tree, then the data frame is accessed as copy_of_my_pruned_tree$frame, and it works just like a normal data frame.).
```{r}
prune.Insurance_antilog$frame$yval <- exp(prune.Insurance_antilog$frame$yval)
```

#### 8.c.iii  After you reverse the log transform on the yval column, then replot the tree with labels.
```{r}
plot(prune.Insurance_antilog)
text(prune.Insurance_antilog, pretty = 0)
```

